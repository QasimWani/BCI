{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOiJG88vy+qHhx7IkdTrkkr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QasimWani/Brain-Computer-Interface/blob/master/TD3_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYbvpyStVXWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "ed3a134c-a3d2-4836-e474-826a68fdf589"
      },
      "source": [
        "!pip install gym==0.15.4\n",
        "!pip install box2d-py\n",
        "import gym\n",
        "print(gym.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym==0.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 31.8MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-cp36-none-any.whl size=1648483 sha256=4558c3e6d890c9695b6e7e153e39889c44c21533e6b722e06654d27b5ed836af\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n",
            "Successfully built gym\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement cloudpickle==1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: gym 0.17.2\n",
            "    Uninstalling gym-0.17.2:\n",
            "      Successfully uninstalled gym-0.17.2\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2\n",
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 4.4MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "0.15.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyXUF4PQQRF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "87e397b3-f0c6-4133-a36b-1b118abbef0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VafmR2lIQXr2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbe79eb7-1ea0-4aba-e1f2-28b897659a8a"
      },
      "source": [
        "cd drive/My\\ Drive/TD3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TD3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXFFTG7nQ9pL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9de802b5-7bd0-47bb-b5af-fe3390074904"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mmodels\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j6clupYOhdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import torch\n",
        "import random\n",
        "\n",
        "\n",
        "#Implemented from: https://github.com/QasimWani/policy-value-methods/blob/master/DDPG/ddpg_agent.py#L150\n",
        "\n",
        "#Set to cuda (gpu) instance if compute available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "BUFFER_SIZE = 50000 #max number of experiences in a buffer\n",
        "MINI_BATCH = 256 #number of samples to collect from buffer\n",
        "\n",
        "class ReplayBuffer():\n",
        "    \"\"\"\n",
        "    Implementation of a fixed size replay buffer as used in DQN algorithms.\n",
        "    The goal of a replay buffer is to unserialize relationships between sequential experiences, gaining a better temporal understanding.\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size=BUFFER_SIZE, batch_size=MINI_BATCH):\n",
        "        \"\"\"\n",
        "        Initializes the buffer.\n",
        "        @Param:\n",
        "        1. action_size: env.action_space.shape[0]\n",
        "        2. buffer_size: Maximum length of the buffer for extrapolating all experiences into trajectories. default - 1e6 (Source: DeepMind)\n",
        "        3. batch_size: size of mini-batch to train on. default = 64.\n",
        "        \"\"\"\n",
        "        self.replay_memory = deque(maxlen=buffer_size) #Experience replay memory object\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"]) #standard S,A,R,S',done\n",
        "        \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adds an experience to existing memory\"\"\"\n",
        "        trajectory = self.experience(state, action, reward, next_state, done)\n",
        "        self.replay_memory.append(trajectory)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly picks minibatches within the replay_buffer of size mini_batch\"\"\"\n",
        "        experiences = random.sample(self.replay_memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):#override default __len__ operator\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.replay_memory)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDa6u5nbNvNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Twin Delayed Deep Deterministic Policy Gradient (TD3) implementation for continuous action space control.\n",
        "#Paper: https://arxiv.org/pdf/1802.09477.pdf\n",
        "#Author: https://github.com/sfujim/TD3\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size, max_action, fc1=256, fc2=256):\n",
        "        \"\"\"\n",
        "        Initializes actor object.\n",
        "        @Param:\n",
        "        1. state_size: env.observation_space.shape[0].\n",
        "        2. action_size: env.action_space.shape[0].\n",
        "        3. max_action: abs(env.action_space.low), sets boundary/clip for policy approximation.\n",
        "        4. fc1: number of hidden units for the first fully connected layer, fc1. Default = 256.\n",
        "        5. fc2: number of hidden units for the second fully connected layer, fc1. Default = 256.\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        #Layer 1\n",
        "        self.fc1 = nn.Linear(state_size, fc1)\n",
        "        #Layer 2\n",
        "        self.fc2 = nn.Linear(fc1, fc2)\n",
        "        #Layer 3\n",
        "        self.mu = nn.Linear(fc2, action_size)\n",
        "\n",
        "        #Define boundary for action space.\n",
        "        self.max_action = max_action\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"Peforms forward pass to map state--> pi(s)\"\"\"\n",
        "        #Layer 1\n",
        "        x = self.fc1(state)\n",
        "        x = F.relu(x)\n",
        "        #Layer 2\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        #Output layer\n",
        "        mu = torch.tanh(self.mu(x))#set action b/w -1 and +1\n",
        "        return self.max_action * mu\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size, fc1=256, fc2=256):\n",
        "        \"\"\"\n",
        "        Initializes Critic object, Q1 and Q2.\n",
        "        Architecture different from DDPG. See paper for full details.\n",
        "        @Param:\n",
        "        1. state_size: env.observation_space.shape[0].\n",
        "        2. action_size: env.action_space.shape[0].\n",
        "        3. fc1: number of hidden units for the first fully connected layer, fc1. Default = 256.\n",
        "        4. fc2: number of hidden units for the second fully connected layer, fc1. Default = 256.\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        #---------Q1 architecture---------\n",
        "        \n",
        "        #Layer 1\n",
        "        self.l1 = nn.Linear(state_size + action_size, fc1)\n",
        "        #Layer 2\n",
        "        self.l2 = nn.Linear(fc1, fc2)\n",
        "        #Output layer\n",
        "        self.l3 = nn.Linear(fc2, 1)#Q-value\n",
        "\n",
        "        #---------Q2 architecture---------\n",
        "\n",
        "        #Layer 1\n",
        "        self.l4 = nn.Linear(state_size + action_size, fc1)\n",
        "        #Layer 2\n",
        "        self.l5 = nn.Linear(fc1, fc2)\n",
        "        #Output layer\n",
        "        self.l6 = nn.Linear(fc2, 1)#Q-value\n",
        "    \n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Perform forward pass by mapping (state, action) --> Q-value\"\"\"\n",
        "        x = torch.cat([state, action], dim=1) #concatenate state and action such that x.shape = state.shape + action.shape\n",
        "\n",
        "        #---------Q1 critic forward pass---------\n",
        "        #Layer 1\n",
        "        q1 = F.relu(self.l1(x))\n",
        "        #Layer 2\n",
        "        q1 = F.relu(self.l2(q1))\n",
        "        #value prediction for Q1\n",
        "        q1 = self.l3(q1)\n",
        "\n",
        "        #---------Q2 critic forward pass---------\n",
        "        #Layer 1\n",
        "        q2 = F.relu(self.l4(x))\n",
        "        #Layer 2\n",
        "        q2 = F.relu(self.l5(q2))\n",
        "        #value prediction for Q2\n",
        "        q2 = self.l6(q2)\n",
        "\n",
        "        return q1, q2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsYasf6wN_4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "#implementation from paper: https://arxiv.org/pdf/1802.09477.pdf\n",
        "#source: https://github.com/sfujim/TD3/blob/master/TD3.py\n",
        "\n",
        "#Set to cuda (gpu) instance if compute available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    \"\"\"Agent that plays and learn from experience. Hyper-paramters chosen from paper.\"\"\"\n",
        "    def __init__(\n",
        "            self, \n",
        "            state_size, \n",
        "            action_size, \n",
        "            max_action, \n",
        "            discount=0.99,\n",
        "            tau=0.005,\n",
        "            policy_noise=0.2,\n",
        "            noise_clip=0.5,\n",
        "            policy_freq=2\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the Agent.\n",
        "        @Param:\n",
        "        1. state_size: env.observation_space.shape[0]\n",
        "        2. action_size: env.action_size.shape[0]\n",
        "        3. max_action: list of max values that the agent can take, i.e. abs(env.action_space.high)\n",
        "        4. discount: return rate\n",
        "        5. tau: soft target update\n",
        "        6. policy_noise: noise reset level, DDPG uses Ornstein-Uhlenbeck process\n",
        "        7. noise_clip: sets boundary for noise calculation to prevent from overestimation of Q-values\n",
        "        8. policy_freq: number of timesteps to update the policy (actor) after\n",
        "        \"\"\"\n",
        "        super(Agent, self).__init__()\n",
        "\n",
        "        #Actor Network initialization\n",
        "        self.actor = Actor(state_size, action_size, max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor) #loads main model into target model\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=0.001)\n",
        "\n",
        "        #Critic Network initialization\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic) #loads main model into target model\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.001)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_freq\n",
        "        self.total_it = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Selects an automatic epsilon-greedy action based on the policy\"\"\"\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "    \n",
        "    def train(self, replay_buffer:ReplayBuffer):\n",
        "        \"\"\"Train the Agent\"\"\"\n",
        "\n",
        "        self.total_it += 1\n",
        "\n",
        "        # Sample replay buffer \n",
        "        state, action, reward, next_state, done = replay_buffer.sample()#sample 256 experiences\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Select action according to policy and add clipped noise\n",
        "            noise = (\n",
        "                torch.randn_like(action) * self.policy_noise\n",
        "            ).clamp(-self.noise_clip, self.noise_clip)\n",
        "            \n",
        "\n",
        "            next_action = (\n",
        "                self.actor_target(next_state) + noise #noise only set in training to prevent from overestimation\n",
        "            ).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action) #Q1, Q2\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + done * self.discount * target_Q #TD-target\n",
        "\n",
        "        # Get current Q estimates\n",
        "        current_Q1, current_Q2 = self.critic(state, action) #Q1, Q2\n",
        "\n",
        "        # Compute critic loss using MSE\n",
        "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "        # Optimize the critic\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Delayed policy updates (DDPG baseline = 1)\n",
        "        if(self.total_it % self.policy_freq == 0):\n",
        "\n",
        "            # Compute actor loss\n",
        "            actor_loss = -self.critic(state, self.actor(state))[0].mean()\n",
        "            \n",
        "            # Optimize the actor \n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # Soft update by updating the frozen target models\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\n",
        "    def save(self, filename):\n",
        "        \"\"\"Saves the Actor Critic local and target models\"\"\"\n",
        "        torch.save(self.critic.state_dict(), \"models/checkpoint/\"+ filename + \"_critic\")\n",
        "        torch.save(self.critic_optimizer.state_dict(), \"models/checkpoint/\" + filename + \"_critic_optimizer\")\n",
        "\n",
        "        torch.save(self.actor.state_dict(), \"models/checkpoint/\" + filename + \"_actor\")\n",
        "        torch.save(self.actor_optimizer.state_dict(), \"models/checkpoint/\" + filename + \"_actor_optimizer\")\n",
        "\n",
        "\n",
        "    def load(self, filename):\n",
        "        \"\"\"Loads the Actor Critic local and target models\"\"\"\n",
        "        self.critic.load_state_dict(torch.load(\"models/checkpoint/\" + filename + \"_critic\"))\n",
        "        self.critic_optimizer.load_state_dict(torch.load(\"models/checkpoint/\" + filename + \"_critic_optimizer\"))\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.actor.load_state_dict(torch.load(\"models/checkpoint/\" + filename + \"_actor\"))\n",
        "        self.actor_optimizer.load_state_dict(torch.load(\"models/checkpoint/\" + filename + \"_actor_optimizer\"))\n",
        "        self.actor_target = copy.deepcopy(self.actor)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDnRTgc6ODBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "939694a1-f50d-48d1-eedf-bb10d79fcda9"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env_id = \"BipedalWalker-v2\"\n",
        "env = gym.make(env_id)\n",
        "\n",
        "\n",
        "#set seeds\n",
        "random_seed = 0\n",
        "env.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "\n",
        "#Set exploration noise for calculating action based on some noise factor\n",
        "exploration_noise = 0.1\n",
        "\n",
        "#Define observation and action space\n",
        "state_space = env.observation_space.shape[0]\n",
        "action_space = env.action_space.shape[0] \n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "#Create Agent\n",
        "policy = Agent(state_space, action_space, max_action)\n",
        "\n",
        "# try:\n",
        "#     policy.load(\"20\")\n",
        "# except:\n",
        "#     raise IOError(\"Couldn't load policy\")\n",
        "\n",
        "#Create Replay Buffer\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "\n",
        "#Train the model\n",
        "max_episodes = 1000\n",
        "max_timesteps = 2000\n",
        "\n",
        "ep_reward = [] #get list of reward for range(max_episodes)\n",
        "\n",
        "for episode in range(1, max_episodes+1):\n",
        "    avg_reward = 0\n",
        "    state = env.reset()\n",
        "    for t in range(1, max_timesteps + 1):\n",
        "        # select action and add exploration noise:\n",
        "        action = policy.select_action(state) + np.random.normal(0, max_action * exploration_noise, size=action_space)\n",
        "        action = action.clip(env.action_space.low, env.action_space.high)\n",
        "            \n",
        "        # take action in env:\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        replay_buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "            \n",
        "        avg_reward += reward\n",
        "\n",
        "        #Renders an episode\n",
        "        # env.render()\n",
        "        if(len(replay_buffer) > 256):#make sure sample is less than overall population\n",
        "            policy.train(replay_buffer) #training mode\n",
        "\n",
        "        # if episode is done then update policy:\n",
        "        if(done or t >=max_timesteps):\n",
        "            print(f\"Episode {episode} reward: {avg_reward} | Rolling average: {np.mean(ep_reward)}\")\n",
        "            print(f\"Current time step: {t}\")\n",
        "            ep_reward.append(avg_reward)\n",
        "            break \n",
        "    \n",
        "    if(np.mean(ep_reward[-10:]) >= 300):\n",
        "          policy.save(\"final\")\n",
        "          break\n",
        "\n",
        "    if(episode % 100 == 0 and episode > 0):\n",
        "        #Save policy and optimizer every 100 episodes\n",
        "        policy.save(str(\"%02d\" % (episode//100)))\n",
        "\n",
        "env.close()\n",
        "\n",
        "#Display Scores\n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(1, len(ep_reward) + 1), ep_reward)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode 1 reward: -125.4405564372999 | Rolling average: nan\n",
            "Current time step: 331\n",
            "Episode 2 reward: -111.61365066014513 | Rolling average: -125.4405564372999\n",
            "Current time step: 60\n",
            "Episode 3 reward: -107.66646206253465 | Rolling average: -118.52710354872252\n",
            "Current time step: 78\n",
            "Episode 4 reward: -126.34562858108363 | Rolling average: -114.90688971999323\n",
            "Current time step: 88\n",
            "Episode 5 reward: -136.08212325088542 | Rolling average: -117.76657443526584\n",
            "Current time step: 375\n",
            "Episode 6 reward: -210.11168341239187 | Rolling average: -121.42968419838977\n",
            "Current time step: 943\n",
            "Episode 7 reward: -114.30465781226322 | Rolling average: -136.21001740072344\n",
            "Current time step: 125\n",
            "Episode 8 reward: -110.85719937897036 | Rolling average: -133.0806803166577\n",
            "Current time step: 37\n",
            "Episode 9 reward: -108.04419271392985 | Rolling average: -130.30274519944678\n",
            "Current time step: 74\n",
            "Episode 10 reward: -105.84954205352483 | Rolling average: -127.829572701056\n",
            "Current time step: 52\n",
            "Episode 11 reward: -106.31622501413403 | Rolling average: -125.63156963630288\n",
            "Current time step: 111\n",
            "Episode 12 reward: -104.19115460709114 | Rolling average: -123.87562921610571\n",
            "Current time step: 106\n",
            "Episode 13 reward: -137.72494105977916 | Rolling average: -122.23525633202117\n",
            "Current time step: 1600\n",
            "Episode 14 reward: -109.07242715458389 | Rolling average: -123.42677054184871\n",
            "Current time step: 111\n",
            "Episode 15 reward: -104.0576388273002 | Rolling average: -122.40146029990123\n",
            "Current time step: 77\n",
            "Episode 16 reward: -114.26242829902735 | Rolling average: -121.1785388683945\n",
            "Current time step: 161\n",
            "Episode 17 reward: -105.29588610934856 | Rolling average: -120.74628195780903\n",
            "Current time step: 81\n",
            "Episode 18 reward: -108.05715098494035 | Rolling average: -119.83743514319372\n",
            "Current time step: 135\n",
            "Episode 19 reward: -109.3819948014029 | Rolling average: -119.18297491217965\n",
            "Current time step: 144\n",
            "Episode 20 reward: -110.09008295421903 | Rolling average: -118.6671338537177\n",
            "Current time step: 138\n",
            "Episode 21 reward: -105.80405744364415 | Rolling average: -118.23828130874276\n",
            "Current time step: 113\n",
            "Episode 22 reward: -117.31177845619268 | Rolling average: -117.64617541040472\n",
            "Current time step: 94\n",
            "Episode 23 reward: -103.91355822028486 | Rolling average: -117.63097554884963\n",
            "Current time step: 116\n",
            "Episode 24 reward: -110.83376177066656 | Rolling average: -117.0345660997816\n",
            "Current time step: 61\n",
            "Episode 25 reward: -103.95790176960318 | Rolling average: -116.77619925273514\n",
            "Current time step: 83\n",
            "Episode 26 reward: -100.96921276880882 | Rolling average: -116.26346735340987\n",
            "Current time step: 76\n",
            "Episode 27 reward: -109.9389381296502 | Rolling average: -115.67522679246369\n",
            "Current time step: 44\n",
            "Episode 28 reward: -109.50341493108104 | Rolling average: -115.46277165680392\n",
            "Current time step: 122\n",
            "Episode 29 reward: -141.15349390378478 | Rolling average: -115.2499374880281\n",
            "Current time step: 1600\n",
            "Episode 30 reward: -165.56556543785408 | Rolling average: -116.14316357133005\n",
            "Current time step: 1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3IyDbsvpzVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}